{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calorimetry with DNNs\n",
    "\n",
    "The data from highly granular calorimeters can be viewed as 3D images, making them ideal for image classification problems. In this lab, we will apply image classification to simulated calorimeter data from the LCD detector concept for the CLIC accelerator. We will use the [CaloDNN](https://github.com/UTA-HEP-Computing/CaloDNN) package to systematically study different neural network architectures, optimizers, loss functions, and other hyperparameters.\n",
    "\n",
    "The data is compused of 4 particle types: electrons, neutral pions (pi0s), charged pions, and photons (gamma). The LCD calorimeter is composed of electromagnetic (ECAL) and hadronic (HCAL) sections. The simulation shoots a single particle into the calorimeter and stores a 25 by 25 by 25 cell part of the ECAL and 5 by 5 by 60 part of the HCAL around the particle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaloDNN\n",
    "\n",
    "CaloDNN is a DLKit based package for studying LCD Calorimetry with DNNs.\n",
    "\n",
    "The package consists of the following files:\n",
    "\n",
    "  **CaloDNN/ClassificationExperiment.py**: This is the “experiment” that drives everything.\n",
    "\n",
    "   **CaloDNN/ClassificationArguments.py**: This is the file where all of the above arguments are defined and parsed. You can add your own options here if need be. Some defaults are defined here.\n",
    "\n",
    "   **CaloDNN/ClassificationScanConfig.py**: This is the configuration file. The model and experiment parameters are set here. This example is setup to allow hyper-parameter scanning. It also contains the list of input files and maps the files and datasets to classes as well as controls what variables are used in the Neural Network.\n",
    "\n",
    "   **CaloDNN/Models.py**: This contains the Keras models, wrapped in a DLKit ModelWrapper class.\n",
    "\n",
    "   **CaloDNN/LCDData.py**: Contains the DLGenerators to read the data.\n",
    "   \n",
    "Typically we run these experiments from the shell command prompt (e.g. here getting help):\n",
    "\n",
    "    python -m CaloDNN.ClassificationExperiment --help\n",
    "    \n",
    "But we can also do it in our current Jupyter session as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -im CaloDNN.ClassificationExperiment -- --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment has 4 steps:\n",
    "\n",
    "    1. Setup Loading Data\n",
    "    2. Load or Build Model\n",
    "    3. Train Model\n",
    "    4. Run Analysis \n",
    "\n",
    "You can turn off steps as needed using the flags above. \n",
    "\n",
    "You can run a quick test (reduced number of events and epochs) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -im CaloDNN.ClassificationExperiment -- --Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and Hyperparameter Scanning\n",
    "\n",
    "Our DNN architecture is defined by the types, number, and dimension of layers. Hyper-parameter\n",
    "scanning refers to the process of searching for an optimal architecture that performs well for a\n",
    "task and can be trained and applied within reasonable time. Beyond the parameters that define the\n",
    "DNN architecture, other configuration parameters allow setting and testing activation and cost\n",
    "functions, optimization (e.g. minimization) techniques, and rate other training parameters.\n",
    "\n",
    "In DLKit, these parameters are set in a configuration file, which defines a single python key/value\n",
    "dictionary called **Config**. DLKit puts the contents of this dictionary in the global scope with the\n",
    "keys as the variable names. As an example, see `CaloDNN/ClassificationScanConfig.py`:\n",
    "\n",
    "```\n",
    "Config={\n",
    "    \"MaxEvents\":int(3.e6),\n",
    "    \"NTestSamples\":100000,\n",
    "    \"NClasses\":4,\n",
    "\n",
    "    \"Epochs\":1000,\n",
    "    \"BatchSize\":1024,\n",
    "\n",
    "...\n",
    "\n",
    "    # Configure Running time callback\n",
    "    # Set RunningTime to a value to stop training after N seconds.\n",
    "    \"RunningTime\": 2*3600,\n",
    "\n",
    "    # Load last trained version of this model configuration. (based on Name var below)\n",
    "    \"LoadPreviousModel\":True\n",
    "    }\n",
    "```\n",
    "\n",
    "These parameters are fixed and will be used by the Experiment to build the model. \n",
    "\n",
    "An important parameter in this configuration file is the `RunningTime`, which sets duration of the training. Using this parameter, you can train a model for a fix amount of time. You can rerun the job to continue training, which will automatically load the last successful training session, as set by `LoadPreviousModel` parameter.\n",
    "\n",
    "[`CaloDNN/ClassificationScanConfig.py`](https://github.com/UTA-HEP-Computing/CaloDNN/blob/master/ClassificationScanConfig.py) is well commented. We suggest you read through the comments.\n",
    "\n",
    "For hyper-parameter scanning, it would be cumbersome to generate a new configuration file for every\n",
    "network we would like to try. Instead, **ScanConfig.py** uses a second dictionary  to specify\n",
    "parameters that you would like to scan, and the **DLTools.Permutator** class to generate all possible\n",
    "resulting configurations. For example the following lines:\n",
    "\n",
    "```\n",
    "# Parameters to scan and their scan points.\n",
    "Params={    \"optimizer\":[\"'RMSprop'\",\"'Adam'\",\"'SGD'\"],\n",
    "            \"Width\":[32,64,128,256,512],\n",
    "            \"Depth\":range(1,5),\n",
    "            \"lr\":[0.01,0.001],\n",
    "            \"decay\":[0.01,0.001],\n",
    "          }\n",
    " ```\n",
    "\n",
    "will generate 3 x 5 x 4 x 2 x 2 = 240 different configurations, which we can enumerate through. To check, we can\n",
    "simply run the **ClassificationScanConfig.py** file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m CaloDNN.ClassificationScanConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should tell you the number possible configurations. We will select\n",
    "specific ones using the **-s** flag when running the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -im CaloDNN.ClassificationExperiment -- --Test -s 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Performing a Scan\n",
    "\n",
    "From above, it should be appearant that in order you can easily try all possible configurations by running the same command with all possible values of the `-s` parameter. And since every configuration is independent, you can run the experiments in parallel. \n",
    "\n",
    "### PBS/Torque Batch System\n",
    "\n",
    "On most GPU equipped clusters, like UTA-DL, a batch system allows you to submit \"jobs\" into \"queues\" which will then execute each job when appropriate resources become available. \n",
    "\n",
    "You can get a list of available queue, using the `qstat -Q` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the UTA-DL cluster, the queues are setup as follows. The `cpu_queue` and `gpu_queue` routing queues send jobs to CPU and GPU resources on each of 5 nodes:\n",
    "\n",
    "    * thecount: 44-core 10 GPU\n",
    "    * super: 24-core 4 GPU\n",
    "    * thingone and thingtwo: 6 core 4 GPU each.\n",
    "    * oscar: 6 core 2 GPU (used for Jupyter sessions).\n",
    "    \n",
    "Submitting to the queue system, requires you to write a script. For example, this is the script `CaloDNN/ScanJob.sh`:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#PBS -V\n",
    "printenv\n",
    "mkdir -p ScanLogs\n",
    "output=ScanLogs/$PBS_ARRAYID.log\n",
    "\n",
    "echo $output >> $output\n",
    "echo Running on $HOSTNAME >> $output\n",
    "echo Array Number: $PBS_ARRAYID >> $output\n",
    "echo Queue: $PBS_QUEUE >> $output\n",
    "\n",
    "cd ~/Tutorial/DLKit\n",
    "source setup.sh\n",
    "\n",
    "python -m CaloDNN.ClassificationExperiment -s $PBS_ARRAYID &>> $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts creates a directory to store the `stdout/stderr` output of the job. Sets up the environment, and starts the job. To set the `-s` parameter, we use Torque's array job mechanism, which will set the `$PBS_ARRAYID` environment variable, to an interger as specified during submission.\n",
    "\n",
    "So for example, to run configurations 10-20, we do (don't run this unless you mean it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -q gpu_queue -t 10-20 CaloDNN/ScanJob.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor your jobs using the `qstat` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "After you jobs start to complete, you can start viewing the performance using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python -m DLAnalysis.Scan TrainedModels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the performance of all of the models in your scan using the python notebook [`CaloDNN/AnalyzeScan-OptimizerStudy.ipynb`](https://github.com/UTA-HEP-Computing/CaloDNN/blob/master/AnalyzeScan-OptimizerStudy.ipynb). Simply make a copy of the notebook into your DLKit directory and navigate Jupyter to the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp CaloDNN/AnalyzeScan-OptimizerStudy.ipynb ./AnalyzeScan-MyStudy.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly use can use [`CaloDNN/AnalyzerPerformance.ipynb`](https://github.com/UTA-HEP-Computing/CaloDNN/blob/master/AnalyzePerformance.ipynb) to study the performance of a specific model in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main driver of the experiment, [`CaloDNN/ClassificationExperiment.py`](https://github.com/UTA-HEP-Computing/CaloDNN/blob/master/ClassificationExperiment.py), is well commented. In order for you to add you own models and modify things, you should carefully read through this file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
